{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eGZIh3o1xch"
      },
      "source": [
        "# GEMM on GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IcE4c-V5Psg"
      },
      "source": [
        "## 1. Set-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bMbgGWP75Psg",
        "outputId": "55dfb7b9-6256-4139-d63d-6da91843ba11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "REpyYw1o5Psi"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Make sure your token is stored in a txt file at the location below.\n",
        "# This way there is no risk that you will push it to your repo\n",
        "# Never share your token with anyone, it is basically your github password!\n",
        "with open('/content/gdrive/MyDrive/ece5545/token.txt') as f:\n",
        "    token = f.readline().strip()\n",
        "# Use another file to store your github username\n",
        "with open('/content/gdrive/MyDrive/ece5545/git_username.txt') as f:\n",
        "    handle = f.readline().strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "xmVrYXr05Psi",
        "outputId": "796313d6-f55b-4cd3-bcb1-8fc9bd13560c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/gdrive/MyDrive/ece5545’: File exists\n",
            "/content/gdrive/MyDrive/ece5545\n",
            "fatal: destination path 'a3-jiayuangu762943' already exists and is not an empty directory.\n",
            "/content/gdrive/MyDrive/ece5545/a3-jiayuangu762943\n",
            "Already on 'main'\n",
            "Your branch is up to date with 'origin/main'.\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 4 (delta 2), reused 4 (delta 2), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects: 100% (4/4), 704 bytes | 2.00 KiB/s, done.\n",
            "From https://github.com/ML-HW-SYS/a3-jiayuangu762943\n",
            "   07be112..bfb1061  main       -> origin/main\n",
            "Updating 07be112..bfb1061\n",
            "Fast-forward\n",
            " src/ops.py | 64 \u001b[32m+++++++++++++++++++++++++\u001b[m\u001b[31m---------------------------------------\u001b[m\n",
            " 1 file changed, 25 insertions(+), 39 deletions(-)\n",
            "/content/gdrive/MyDrive/ece5545\n"
          ]
        }
      ],
      "source": [
        "# Clone your github repo\n",
        "YOUR_TOKEN = token\n",
        "YOUR_HANDLE = handle\n",
        "BRANCH = \"main\"\n",
        "\n",
        "%mkdir /content/gdrive/MyDrive/ece5545\n",
        "%cd /content/gdrive/MyDrive/ece5545\n",
        "!git clone https://{YOUR_TOKEN}@github.com/ML-HW-SYS/a3-{YOUR_HANDLE}.git\n",
        "%cd /content/gdrive/MyDrive/ece5545/a3-{YOUR_HANDLE}\n",
        "!git checkout {BRANCH}\n",
        "!git pull\n",
        "%cd /content/gdrive/MyDrive/ece5545\n",
        "\n",
        "PROJECT_ROOT = f\"/content/gdrive/MyDrive/ece5545/a3-{YOUR_HANDLE}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Ll4LScr_5Psi"
      },
      "outputs": [],
      "source": [
        "# # This extension reloads all imports before running each cell\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "WdiSdTlu5Psj",
        "outputId": "95266640-28e3-4a20-82c2-9c7b3ebd4000",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1-conv1d_cpu.ipynb  3-conv1d_fpga.ipynb  5-conv2d_dw_gpu.ipynb\tREADME.md  tests\n",
            "2-conv1d_gpu.ipynb  4-gemm_gpu.ipynb\t leaderboard_id.txt\tsrc\n"
          ]
        }
      ],
      "source": [
        "!ls {PROJECT_ROOT}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJDlW3_V5uX8"
      },
      "source": [
        "## 2. Install TVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VkqX6TEG5tz7",
        "outputId": "97b96cc1-5ff5-41bd-e069-ba073c346431",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://tlcpack.ai/wheels\n",
            "Collecting tlcpack-nightly-cu102\n",
            "  Downloading https://github.com/tlc-pack/tlcpack/releases/download/v0.12.dev/tlcpack_nightly_cu102-0.15.dev118%2Bg51bdaec6e-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (428.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m428.5/428.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.11/dist-packages (from tlcpack-nightly-cu102) (25.3.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from tlcpack-nightly-cu102) (3.1.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from tlcpack-nightly-cu102) (4.4.2)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from tlcpack-nightly-cu102) (0.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from tlcpack-nightly-cu102) (1.24.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from tlcpack-nightly-cu102) (5.9.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from tlcpack-nightly-cu102) (1.14.1)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.11/dist-packages (from tlcpack-nightly-cu102) (6.4.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from tlcpack-nightly-cu102) (4.13.1)\n",
            "Installing collected packages: tlcpack-nightly-cu102\n",
            "Successfully installed tlcpack-nightly-cu102-0.15.dev118+g51bdaec6e\n"
          ]
        }
      ],
      "source": [
        "!pip install tlcpack-nightly-cu102 -f https://tlcpack.ai/wheels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16GZ_We05Psj"
      },
      "source": [
        "## 3. Check the implementation of `make_gemm_gpu_scheduler` function in `src.ops`\n",
        "\n",
        "The function implements General Matrix Multiply (GEMM) on GPU. You should use TVM to optimize it.\n",
        "\n",
        "Let $A \\in \\mathbb{R}^{m \\times k}$, $W \\in \\mathbb{R}^{k \\times n}$, and $B \\in \\mathbb{R}^{m \\times n}$, then\n",
        "$$\n",
        "B = A \\times W\n",
        "$$\n",
        "Please see the numpy matmul function for more detail: [link](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html).\n",
        "\n",
        "The `make_gemm_gpu_scheduler` takes $m$, $k$, and $n$. The first matrix is $m \\times k$, the second matrix is $k \\times n$, and the output matrix is $m \\times n$.\n",
        "\n",
        "The function returns both the TVM scheduler and the TVM opterator for\n",
        "1. Input $a$\n",
        "2. Input $w$\n",
        "3. Output $b$\n",
        "\n",
        "The scheduler should be able to used to build a function with signature $func(a, w, b)$.\n",
        "Please see the following cells for usage."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.24.3"
      ],
      "metadata": {
        "id": "wraL6GBiFtef",
        "outputId": "45dd469c-ebd3-4e64-f5e0-44de2226d943",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.24.3\n",
            "  Downloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Downloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
            "pymc 5.21.2 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\n",
            "blosc2 3.2.1 requires numpy>=1.26, but you have numpy 1.24.3 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.3 which is incompatible.\n",
            "albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "47236bbecc0d4c539409ca25e308a778"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vanilla Code"
      ],
      "metadata": {
        "id": "xgaxnVRNi2om"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tvm\n",
        "from tvm import te\n",
        "def vanilla_make_gemm_gpu_scheduler(M, K, N):\n",
        "     A = te.placeholder((M, K), name=\"A\")\n",
        "     B = te.placeholder((K, N), name=\"B\")\n",
        "\n",
        "     # TVM Matrix Multiplication using TE\n",
        "     k = te.reduce_axis((0, K), \"k\")\n",
        "     A = te.placeholder((M, K), name=\"A\")\n",
        "     B = te.placeholder((K, N), name=\"B\")\n",
        "     C = te.compute((M, N), lambda x, y: te.sum(A[x, k] * B[k, y], axis=k), name=\"C\")\n",
        "     # Default schedule\n",
        "     s = te.create_schedule(C.op)\n",
        "\n",
        "     # the i-th block is indexed by blockIdx.x.\n",
        "     # the number of threads in each block is blockDim.x\n",
        "     # and the i-th thread within a block is indexed by threadIdx.x\n",
        "     # overall index of a thread can be calculated as\n",
        "     # 𝑖=blockIdx.x×blockDim.x+threadIdx.x\n",
        "     block_x = te.thread_axis(\"blockIdx.y\")\n",
        "     block_y = te.thread_axis(\"blockIdx.x\")\n",
        "\n",
        "     x, y = s[C].op.axis\n",
        "     (k,) = s[C].op.reduce_axis\n",
        "     s[C].bind(y, block_y)\n",
        "     s[C].bind(x, block_x)\n",
        "\n",
        "     return s, A, B, C\n",
        ""
      ],
      "metadata": {
        "id": "TiSFXz5Yi4Cl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vanilla runtime"
      ],
      "metadata": {
        "id": "uRI3KAhxjBar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tvm\n",
        "import numpy as np\n",
        "import sys\n",
        "# Adding assignment 3 to the system path\n",
        "# Make sure this matches your git directory\n",
        "sys.path.insert(0, PROJECT_ROOT)\n",
        "\n",
        "M = 1024\n",
        "N = 512\n",
        "K = 2048\n",
        "dtype = 'float32'\n",
        "a_np = np.random.rand(M, K).astype(dtype)\n",
        "w_np = np.random.rand(K, N).astype(dtype)\n",
        "b_np = np.matmul(a_np, w_np)\n",
        "\n",
        "s, A, W, B = vanilla_make_gemm_gpu_scheduler(M, K, N)\n",
        "func = tvm.build(s, [A, W, B], \"cuda\")\n",
        "\n",
        "dev = tvm.cuda(0)\n",
        "a = tvm.nd.array(a_np, dev)\n",
        "w = tvm.nd.array(w_np, dev)\n",
        "b = tvm.nd.array(np.zeros((M, N), dtype), dev)\n",
        "func(a, w, b)\n",
        "evaluator = func.time_evaluator(func.entry_name, dev, number=1, repeat =1)\n",
        "\n",
        "\n",
        "print(\"Answer:\", b_np)\n",
        "print(\"Output:\", b)\n",
        "print(f\"GEMM TVM: %f ms\" % (evaluator(a, w, b).mean * 1e3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PI1yJkeWjDCH",
        "outputId": "55a4d507-d114-4eda-902d-70f01e457631"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: [[512.28424 511.44678 507.24255 ... 499.75528 507.71405 519.5174 ]\n",
            " [513.6171  501.62268 502.89038 ... 497.90283 498.33096 506.67056]\n",
            " [509.03876 503.462   507.72232 ... 502.51074 506.05255 517.33453]\n",
            " ...\n",
            " [501.40363 495.00513 499.86172 ... 494.20172 490.54358 505.06915]\n",
            " [523.2278  514.9554  512.5181  ... 520.0199  519.0525  523.253  ]\n",
            " [529.64685 522.3683  526.1968  ... 517.64026 526.00134 530.5718 ]]\n",
            "Output: [[512.284   511.44635 507.24258 ... 499.75513 507.71426 519.51685]\n",
            " [513.6173  501.623   502.89032 ... 497.90326 498.33084 506.67038]\n",
            " [509.03873 503.46185 507.72217 ... 502.5105  506.05194 517.3344 ]\n",
            " ...\n",
            " [501.40335 495.0052  499.8619  ... 494.20148 490.54312 505.0691 ]\n",
            " [523.22815 514.95575 512.5178  ... 520.01996 519.0525  523.25323]\n",
            " [529.64636 522.3688  526.1965  ... 517.6405  526.0016  530.5715 ]]\n",
            "GEMM TVM: 87.990272 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimized runtime"
      ],
      "metadata": {
        "id": "awpbbktYi7fL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "2nIPEBHF5Psj",
        "outputId": "fc19c4f9-8e99-491d-f43e-4bcd5d802dac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: [[494.05237 497.4322  485.24713 ... 504.18054 489.75583 506.187  ]\n",
            " [488.38092 493.2765  482.4443  ... 497.54932 485.25323 500.66626]\n",
            " [496.98538 507.87112 492.7254  ... 520.0741  498.0563  512.6229 ]\n",
            " ...\n",
            " [498.74222 516.837   508.20953 ... 519.2613  501.58072 524.17676]\n",
            " [494.94547 513.0723  495.0604  ... 517.2201  498.5652  516.5736 ]\n",
            " [504.20526 514.8947  498.87036 ... 514.85266 504.3047  521.3501 ]]\n",
            "Output: [[494.05203 497.43213 485.24744 ... 504.18088 489.75568 506.18747]\n",
            " [488.3813  493.27716 482.44363 ... 497.5492  485.25354 500.66586]\n",
            " [496.98566 507.87027 492.72528 ... 520.0746  498.05585 512.62286]\n",
            " ...\n",
            " [498.74185 516.837   508.2094  ... 519.2614  501.5805  524.17633]\n",
            " [494.9458  513.0728  495.06067 ... 517.2206  498.56494 516.57404]\n",
            " [504.20535 514.8949  498.8707  ... 514.85284 504.30518 521.3501 ]]\n",
            "GEMM TVM: 72.050689 ms\n"
          ]
        }
      ],
      "source": [
        "import tvm\n",
        "import numpy as np\n",
        "import sys\n",
        "# Adding assignment 3 to the system path\n",
        "# Make sure this matches your git directory\n",
        "sys.path.insert(0, PROJECT_ROOT)\n",
        "from src.ops import make_gemm_gpu_scheduler\n",
        "\n",
        "M = 1024\n",
        "N = 512\n",
        "K = 2048\n",
        "dtype = 'float32'\n",
        "a_np = np.random.rand(M, K).astype(dtype)\n",
        "w_np = np.random.rand(K, N).astype(dtype)\n",
        "b_np = np.matmul(a_np, w_np)\n",
        "\n",
        "s, A, W, B = make_gemm_gpu_scheduler(M, K, N)\n",
        "func = tvm.build(s, [A, W, B], \"cuda\")\n",
        "\n",
        "dev = tvm.cuda(0)\n",
        "a = tvm.nd.array(a_np, dev)\n",
        "w = tvm.nd.array(w_np, dev)\n",
        "b = tvm.nd.array(np.zeros((M, N), dtype), dev)\n",
        "func(a, w, b)\n",
        "evaluator = func.time_evaluator(func.entry_name, dev, number=1, repeat =1)\n",
        "\n",
        "\n",
        "print(\"Answer:\", b_np)\n",
        "print(\"Output:\", b)\n",
        "print(f\"GEMM TVM: %f ms\" % (evaluator(a, w, b).mean * 1e3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "bjnf_oPi5Psk",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5230f09-f14e-488f-a7b5-f10872a02a83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# from tvm.script import ir as I\n",
            "# from tvm.script import tir as T\n",
            "\n",
            "@I.ir_module\n",
            "class Module:\n",
            "    @T.prim_func\n",
            "    def main(A: T.Buffer((1024, 2048), \"float32\"), B: T.Buffer((2048, 512), \"float32\"), C: T.Buffer((1024, 512), \"float32\")):\n",
            "        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n",
            "        blockIdx_x = T.launch_thread(\"blockIdx.x\", 32)\n",
            "        blockIdx_y = T.launch_thread(\"blockIdx.y\", 16)\n",
            "        threadIdx_x = T.env_thread(\"threadIdx.x\")\n",
            "        threadIdx_y = T.env_thread(\"threadIdx.y\")\n",
            "        C_1 = T.Buffer((524288,), data=C.data)\n",
            "        with T.launch_thread(threadIdx_x, 32):\n",
            "            T.launch_thread(threadIdx_y, 32)\n",
            "            C_1[blockIdx_x * 16384 + threadIdx_x * 512 + blockIdx_y * 32 + threadIdx_y] = T.float32(0)\n",
            "        for k_outer in range(512):\n",
            "            T.launch_thread(threadIdx_x, 32)\n",
            "            T.launch_thread(threadIdx_y, 32)\n",
            "            for k_inner in range(4):\n",
            "                A_1 = T.Buffer((2097152,), data=A.data)\n",
            "                B_1 = T.Buffer((1048576,), data=B.data)\n",
            "                C_1[blockIdx_x * 16384 + threadIdx_x * 512 + blockIdx_y * 32 + threadIdx_y] = C_1[blockIdx_x * 16384 + threadIdx_x * 512 + blockIdx_y * 32 + threadIdx_y] + A_1[blockIdx_x * 65536 + threadIdx_x * 2048 + k_outer * 4 + k_inner] * B_1[k_outer * 2048 + k_inner * 512 + blockIdx_y * 32 + threadIdx_y]\n"
          ]
        }
      ],
      "source": [
        "print(tvm.lower(s, [A, W, B], simple_mode=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "VQiasz7n5Psk",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f333480-32fa-40b0-f8b0-cd3e480a15af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/ece5545/a3-jiayuangu762943\n",
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.11.11, pytest-8.3.5, pluggy-1.5.0\n",
            "rootdir: /content/gdrive/MyDrive/ece5545/a3-jiayuangu762943\n",
            "plugins: anyio-4.9.0, typeguard-4.4.2, langsmith-0.3.23\n",
            "collected 29 items                                                                                 \u001b[0m\n",
            "\n",
            "tests/test_gemm_gpu.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                         [100%]\u001b[0m\n",
            "\n",
            "============================================= FAILURES =============================================\n",
            "\u001b[31m\u001b[1m_____________________________________ test1_speed_torch[2000] ______________________________________\u001b[0m\n",
            "\n",
            "execution_number = 2000\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[33m'\u001b[39;49;00m\u001b[33mexecution_number\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m10\u001b[39;49;00m, \u001b[94m100\u001b[39;49;00m, \u001b[94m1000\u001b[39;49;00m, \u001b[94m2000\u001b[39;49;00m, \u001b[94m4000\u001b[39;49;00m, \u001b[94m6000\u001b[39;49;00m, \u001b[94m8000\u001b[39;49;00m, \u001b[94m10000\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest1_speed_torch\u001b[39;49;00m(execution_number):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# Define dimension\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        M = \u001b[94m1024\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        K = \u001b[94m1024\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        N = \u001b[94m1024\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        n_repeat = \u001b[94m100\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# Create random test data\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(seed=\u001b[94m1024\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        a_np = np.random.rand(M, K).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        w_np = np.random.rand(K, N).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# Torch input\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        a_torch = torch.tensor(a_np).float()\u001b[90m\u001b[39;49;00m\n",
            "        w_torch = torch.tensor(w_np).float()\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# Time the torch implementation\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mdef\u001b[39;49;00m \u001b[92mtorch_time\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
            "            ans_torch(a_torch, w_torch)\u001b[90m\u001b[39;49;00m\n",
            "        time_torch = timeit.timeit(torch_time, number=n_repeat)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# Time the optimized implementation\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        func = make_func(M, K, N)\u001b[90m\u001b[39;49;00m\n",
            "        a = tvm.nd.array(a_np, dev)\u001b[90m\u001b[39;49;00m\n",
            "        w = tvm.nd.array(w_np, dev)\u001b[90m\u001b[39;49;00m\n",
            "        b = tvm.nd.array(np.zeros((M, N), dtype=\u001b[33m'\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), dev)\u001b[90m\u001b[39;49;00m\n",
            "        func(a, w, b)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mdef\u001b[39;49;00m \u001b[92mtvm_time\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
            "            func(a, w, b)\u001b[90m\u001b[39;49;00m\n",
            "        time_tvm = timeit.timeit(tvm_time, number=n_repeat)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        opt_folds = \u001b[96mfloat\u001b[39;49;00m(execution_number)\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94massert\u001b[39;49;00m time_tvm * opt_folds <= time_torch, \\\n",
            "            \u001b[33m\"\u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33mx speed-up failed: TVM Time: \u001b[39;49;00m\u001b[33m%.5e\u001b[39;49;00m\u001b[33ms TorchTime: \u001b[39;49;00m\u001b[33m%.5e\u001b[39;49;00m\u001b[33ms\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \\\n",
            "            % (execution_number, time_tvm, time_torch, )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       AssertionError: 2000x speed-up failed: TVM Time: 1.81719e-03s TorchTime: 1.90207e+00s\u001b[0m\n",
            "\u001b[1m\u001b[31mE       assert (0.0018171850006183377 * 2000.0) <= 1.9020662800003265\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_gemm_gpu.py\u001b[0m:163: AssertionError\n",
            "\u001b[31m\u001b[1m_____________________________________ test1_speed_torch[6000] ______________________________________\u001b[0m\n",
            "\n",
            "execution_number = 6000\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[33m'\u001b[39;49;00m\u001b[33mexecution_number\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m10\u001b[39;49;00m, \u001b[94m100\u001b[39;49;00m, \u001b[94m1000\u001b[39;49;00m, \u001b[94m2000\u001b[39;49;00m, \u001b[94m4000\u001b[39;49;00m, \u001b[94m6000\u001b[39;49;00m, \u001b[94m8000\u001b[39;49;00m, \u001b[94m10000\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest1_speed_torch\u001b[39;49;00m(execution_number):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# Define dimension\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        M = \u001b[94m1024\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        K = \u001b[94m1024\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        N = \u001b[94m1024\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        n_repeat = \u001b[94m100\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# Create random test data\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(seed=\u001b[94m1024\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        a_np = np.random.rand(M, K).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        w_np = np.random.rand(K, N).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# Torch input\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        a_torch = torch.tensor(a_np).float()\u001b[90m\u001b[39;49;00m\n",
            "        w_torch = torch.tensor(w_np).float()\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# Time the torch implementation\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mdef\u001b[39;49;00m \u001b[92mtorch_time\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
            "            ans_torch(a_torch, w_torch)\u001b[90m\u001b[39;49;00m\n",
            "        time_torch = timeit.timeit(torch_time, number=n_repeat)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# Time the optimized implementation\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        func = make_func(M, K, N)\u001b[90m\u001b[39;49;00m\n",
            "        a = tvm.nd.array(a_np, dev)\u001b[90m\u001b[39;49;00m\n",
            "        w = tvm.nd.array(w_np, dev)\u001b[90m\u001b[39;49;00m\n",
            "        b = tvm.nd.array(np.zeros((M, N), dtype=\u001b[33m'\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), dev)\u001b[90m\u001b[39;49;00m\n",
            "        func(a, w, b)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mdef\u001b[39;49;00m \u001b[92mtvm_time\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
            "            func(a, w, b)\u001b[90m\u001b[39;49;00m\n",
            "        time_tvm = timeit.timeit(tvm_time, number=n_repeat)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        opt_folds = \u001b[96mfloat\u001b[39;49;00m(execution_number)\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94massert\u001b[39;49;00m time_tvm * opt_folds <= time_torch, \\\n",
            "            \u001b[33m\"\u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33mx speed-up failed: TVM Time: \u001b[39;49;00m\u001b[33m%.5e\u001b[39;49;00m\u001b[33ms TorchTime: \u001b[39;49;00m\u001b[33m%.5e\u001b[39;49;00m\u001b[33ms\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \\\n",
            "            % (execution_number, time_tvm, time_torch, )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       AssertionError: 6000x speed-up failed: TVM Time: 1.24099e-03s TorchTime: 1.76230e+00s\u001b[0m\n",
            "\u001b[1m\u001b[31mE       assert (0.0012409939999997732 * 6000.0) <= 1.7622955289998572\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_gemm_gpu.py\u001b[0m:163: AssertionError\n",
            "\u001b[31m\u001b[1m_____________________________________ test1_speed_torch[8000] ______________________________________\u001b[0m\n",
            "\n",
            "execution_number = 8000\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[33m'\u001b[39;49;00m\u001b[33mexecution_number\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m10\u001b[39;49;00m, \u001b[94m100\u001b[39;49;00m, \u001b[94m1000\u001b[39;49;00m, \u001b[94m2000\u001b[39;49;00m, \u001b[94m4000\u001b[39;49;00m, \u001b[94m6000\u001b[39;49;00m, \u001b[94m8000\u001b[39;49;00m, \u001b[94m10000\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest1_speed_torch\u001b[39;49;00m(execution_number):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# Define dimension\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        M = \u001b[94m1024\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        K = \u001b[94m1024\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        N = \u001b[94m1024\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        n_repeat = \u001b[94m100\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# Create random test data\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(seed=\u001b[94m1024\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        a_np = np.random.rand(M, K).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        w_np = np.random.rand(K, N).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# Torch input\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        a_torch = torch.tensor(a_np).float()\u001b[90m\u001b[39;49;00m\n",
            "        w_torch = torch.tensor(w_np).float()\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# Time the torch implementation\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mdef\u001b[39;49;00m \u001b[92mtorch_time\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
            "            ans_torch(a_torch, w_torch)\u001b[90m\u001b[39;49;00m\n",
            "        time_torch = timeit.timeit(torch_time, number=n_repeat)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# Time the optimized implementation\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        func = make_func(M, K, N)\u001b[90m\u001b[39;49;00m\n",
            "        a = tvm.nd.array(a_np, dev)\u001b[90m\u001b[39;49;00m\n",
            "        w = tvm.nd.array(w_np, dev)\u001b[90m\u001b[39;49;00m\n",
            "        b = tvm.nd.array(np.zeros((M, N), dtype=\u001b[33m'\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), dev)\u001b[90m\u001b[39;49;00m\n",
            "        func(a, w, b)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mdef\u001b[39;49;00m \u001b[92mtvm_time\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
            "            func(a, w, b)\u001b[90m\u001b[39;49;00m\n",
            "        time_tvm = timeit.timeit(tvm_time, number=n_repeat)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        opt_folds = \u001b[96mfloat\u001b[39;49;00m(execution_number)\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94massert\u001b[39;49;00m time_tvm * opt_folds <= time_torch, \\\n",
            "            \u001b[33m\"\u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33mx speed-up failed: TVM Time: \u001b[39;49;00m\u001b[33m%.5e\u001b[39;49;00m\u001b[33ms TorchTime: \u001b[39;49;00m\u001b[33m%.5e\u001b[39;49;00m\u001b[33ms\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \\\n",
            "            % (execution_number, time_tvm, time_torch, )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       AssertionError: 8000x speed-up failed: TVM Time: 1.08742e-03s TorchTime: 5.33643e+00s\u001b[0m\n",
            "\u001b[1m\u001b[31mE       assert (0.001087423999706516 * 8000.0) <= 5.336431898999763\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_gemm_gpu.py\u001b[0m:163: AssertionError\n",
            "\u001b[31m\u001b[1m_____________________________________ test1_speed_torch[10000] _____________________________________\u001b[0m\n",
            "\n",
            "execution_number = 10000\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[33m'\u001b[39;49;00m\u001b[33mexecution_number\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m10\u001b[39;49;00m, \u001b[94m100\u001b[39;49;00m, \u001b[94m1000\u001b[39;49;00m, \u001b[94m2000\u001b[39;49;00m, \u001b[94m4000\u001b[39;49;00m, \u001b[94m6000\u001b[39;49;00m, \u001b[94m8000\u001b[39;49;00m, \u001b[94m10000\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest1_speed_torch\u001b[39;49;00m(execution_number):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# Define dimension\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        M = \u001b[94m1024\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        K = \u001b[94m1024\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        N = \u001b[94m1024\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        n_repeat = \u001b[94m100\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# Create random test data\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(seed=\u001b[94m1024\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        a_np = np.random.rand(M, K).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        w_np = np.random.rand(K, N).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# Torch input\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        a_torch = torch.tensor(a_np).float()\u001b[90m\u001b[39;49;00m\n",
            "        w_torch = torch.tensor(w_np).float()\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# Time the torch implementation\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mdef\u001b[39;49;00m \u001b[92mtorch_time\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
            "            ans_torch(a_torch, w_torch)\u001b[90m\u001b[39;49;00m\n",
            "        time_torch = timeit.timeit(torch_time, number=n_repeat)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# Time the optimized implementation\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        func = make_func(M, K, N)\u001b[90m\u001b[39;49;00m\n",
            "        a = tvm.nd.array(a_np, dev)\u001b[90m\u001b[39;49;00m\n",
            "        w = tvm.nd.array(w_np, dev)\u001b[90m\u001b[39;49;00m\n",
            "        b = tvm.nd.array(np.zeros((M, N), dtype=\u001b[33m'\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), dev)\u001b[90m\u001b[39;49;00m\n",
            "        func(a, w, b)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mdef\u001b[39;49;00m \u001b[92mtvm_time\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
            "            func(a, w, b)\u001b[90m\u001b[39;49;00m\n",
            "        time_tvm = timeit.timeit(tvm_time, number=n_repeat)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        opt_folds = \u001b[96mfloat\u001b[39;49;00m(execution_number)\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94massert\u001b[39;49;00m time_tvm * opt_folds <= time_torch, \\\n",
            "            \u001b[33m\"\u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33mx speed-up failed: TVM Time: \u001b[39;49;00m\u001b[33m%.5e\u001b[39;49;00m\u001b[33ms TorchTime: \u001b[39;49;00m\u001b[33m%.5e\u001b[39;49;00m\u001b[33ms\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \\\n",
            "            % (execution_number, time_tvm, time_torch, )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       AssertionError: 10000x speed-up failed: TVM Time: 1.21571e-03s TorchTime: 1.62326e+00s\u001b[0m\n",
            "\u001b[1m\u001b[31mE       assert (0.0012157070004832349 * 10000.0) <= 1.6232640020007238\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_gemm_gpu.py\u001b[0m:163: AssertionError\n",
            "\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n",
            "\u001b[31mFAILED\u001b[0m tests/test_gemm_gpu.py::\u001b[1mtest1_speed_torch[2000]\u001b[0m - AssertionError: 2000x speed-up failed: TVM Time: 1.81719e-03s TorchTime: 1.90207e+00s\n",
            "\u001b[31mFAILED\u001b[0m tests/test_gemm_gpu.py::\u001b[1mtest1_speed_torch[6000]\u001b[0m - AssertionError: 6000x speed-up failed: TVM Time: 1.24099e-03s TorchTime: 1.76230e+00s\n",
            "\u001b[31mFAILED\u001b[0m tests/test_gemm_gpu.py::\u001b[1mtest1_speed_torch[8000]\u001b[0m - AssertionError: 8000x speed-up failed: TVM Time: 1.08742e-03s TorchTime: 5.33643e+00s\n",
            "\u001b[31mFAILED\u001b[0m tests/test_gemm_gpu.py::\u001b[1mtest1_speed_torch[10000]\u001b[0m - AssertionError: 10000x speed-up failed: TVM Time: 1.21571e-03s TorchTime: 1.62326e+00s\n",
            "\u001b[31m============================= \u001b[31m\u001b[1m4 failed\u001b[0m, \u001b[32m25 passed\u001b[0m\u001b[31m in 70.55s (0:01:10)\u001b[0m\u001b[31m ==============================\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "%cd {PROJECT_ROOT}\n",
        "!python -m pytest tests/test_gemm_gpu.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kdvIYRwjnhBL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "4-gemm_gpu.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}